{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mxception\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocess_input\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=4'>5</a>\u001b[0m train_gen \u001b[39m=\u001b[39m ImageDataGenerator(preprocessing_function\u001b[39m=\u001b[39mpreprocess_input)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=6'>7</a>\u001b[0m train_ds \u001b[39m=\u001b[39m train_gen\u001b[39m.\u001b[39;49mflow_from_directory(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=7'>8</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m./dataset/train\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=8'>9</a>\u001b[0m     target_size\u001b[39m=\u001b[39;49m(\u001b[39m350\u001b[39;49m, \u001b[39m350\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=9'>10</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=12'>13</a>\u001b[0m val_gen \u001b[39m=\u001b[39m ImageDataGenerator(preprocessing_function\u001b[39m=\u001b[39mpreprocess_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=14'>15</a>\u001b[0m val_ds \u001b[39m=\u001b[39m val_gen\u001b[39m.\u001b[39mflow_from_directory(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m./dataset/val\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=16'>17</a>\u001b[0m     target_size\u001b[39m=\u001b[39m(\u001b[39m350\u001b[39m, \u001b[39m350\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=17'>18</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kenwu/Documents/Github/IITB-Deep-Learning-Bootcamp/notebook.ipynb#ch0000002?line=18'>19</a>\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.8/site-packages/keras/preprocessing/image.py:976\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflow_from_directory\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    899\u001b[0m                         directory,\n\u001b[1;32m    900\u001b[0m                         target_size\u001b[39m=\u001b[39m(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m                         subset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    912\u001b[0m                         interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    913\u001b[0m   \u001b[39m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \n\u001b[1;32m    915\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39m          and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 976\u001b[0m   \u001b[39mreturn\u001b[39;00m DirectoryIterator(\n\u001b[1;32m    977\u001b[0m       directory,\n\u001b[1;32m    978\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    979\u001b[0m       target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[1;32m    980\u001b[0m       color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[1;32m    981\u001b[0m       classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[1;32m    982\u001b[0m       class_mode\u001b[39m=\u001b[39;49mclass_mode,\n\u001b[1;32m    983\u001b[0m       data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[1;32m    984\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    985\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    986\u001b[0m       seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    987\u001b[0m       save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[1;32m    988\u001b[0m       save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[1;32m    989\u001b[0m       save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m    990\u001b[0m       follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[1;32m    991\u001b[0m       subset\u001b[39m=\u001b[39;49msubset,\n\u001b[1;32m    992\u001b[0m       interpolation\u001b[39m=\u001b[39;49minterpolation)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.8/site-packages/keras/preprocessing/image.py:394\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    392\u001b[0m     dtype \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mfloatx()\n\u001b[1;32m    393\u001b[0m   kwargs[\u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dtype\n\u001b[0;32m--> 394\u001b[0m \u001b[39msuper\u001b[39;49m(DirectoryIterator, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    395\u001b[0m     directory, image_data_generator,\n\u001b[1;32m    396\u001b[0m     target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[1;32m    397\u001b[0m     color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[1;32m    398\u001b[0m     classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[1;32m    399\u001b[0m     class_mode\u001b[39m=\u001b[39;49mclass_mode,\n\u001b[1;32m    400\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    401\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    402\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    403\u001b[0m     data_format\u001b[39m=\u001b[39;49mdata_format,\n\u001b[1;32m    404\u001b[0m     save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[1;32m    405\u001b[0m     save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[1;32m    406\u001b[0m     save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m    407\u001b[0m     follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[1;32m    408\u001b[0m     subset\u001b[39m=\u001b[39;49msubset,\n\u001b[1;32m    409\u001b[0m     interpolation\u001b[39m=\u001b[39;49minterpolation,\n\u001b[1;32m    410\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.8/site-packages/keras_preprocessing/image/directory_iterator.py:115\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m    114\u001b[0m     classes \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[1;32m    116\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    117\u001b[0m             classes\u001b[39m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/train'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    './dataset/train',\n",
    "    target_size=(350, 350),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    './dataset/val',\n",
    "    target_size=(350, 350),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 22:22:54.918312: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-06 22:22:54.918345: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-06 22:22:54.918370: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (kenwu-Lenovo-Z51-70): /proc/driver/nvidia/version does not exist\n",
      "2021-12-06 22:22:54.918652: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 488s 12s/step - loss: 0.4700 - accuracy: 0.7815 - val_loss: 0.3154 - val_accuracy: 0.8428\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 480s 12s/step - loss: 0.2804 - accuracy: 0.8731 - val_loss: 0.2665 - val_accuracy: 0.8742\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 483s 12s/step - loss: 0.2469 - accuracy: 0.8880 - val_loss: 0.3488 - val_accuracy: 0.8428\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 476s 12s/step - loss: 0.2183 - accuracy: 0.9146 - val_loss: 0.3202 - val_accuracy: 0.8553\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 478s 12s/step - loss: 0.2027 - accuracy: 0.9154 - val_loss: 0.3599 - val_accuracy: 0.8679\n"
     ]
    }
   ],
   "source": [
    "base_model = Xception(\n",
    "    include_top=False,\n",
    "    input_shape=x.shape\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=x.shape)\n",
    "\n",
    "base = base_model(inputs, training=False)\n",
    "\n",
    "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
    "\n",
    "outputs = keras.layers.Dense(2)(vectors)\n",
    "\n",
    "xception_model = keras.Model(inputs, outputs)\n",
    "learning_rate = 0.01\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "xception_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "xception_history = xception_model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation - Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1277 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 494s 12s/step - loss: 0.2945 - accuracy: 0.8677 - val_loss: 0.2976 - val_accuracy: 0.8742\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 488s 12s/step - loss: 0.2530 - accuracy: 0.8841 - val_loss: 0.2547 - val_accuracy: 0.8868\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 484s 12s/step - loss: 0.2807 - accuracy: 0.8872 - val_loss: 0.2628 - val_accuracy: 0.8868\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 491s 12s/step - loss: 0.2365 - accuracy: 0.8912 - val_loss: 0.2663 - val_accuracy: 0.8805\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 486s 12s/step - loss: 0.2330 - accuracy: 0.8990 - val_loss: 0.2876 - val_accuracy: 0.8742\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    './dataset/train',\n",
    "    target_size=(350, 350),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "xception_history = xception_model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 161 images belonging to 2 classes.\n",
      "6/6 [==============================] - 54s 8s/step - loss: 0.2557 - accuracy: 0.9006\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_ds = test_gen.flow_from_directory(\n",
    "    './dataset/test',\n",
    "    target_size=(350, 350),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "xception_score = xception_model.evaluate(test_ds)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:45:39.267889: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1003520000 exceeds 10% of free system memory.\n",
      "2021-12-06 23:45:39.997861: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1003520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/40 [..............................] - ETA: 17:23 - loss: 0.7261 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:46:03.285646: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1003520000 exceeds 10% of free system memory.\n",
      "2021-12-06 23:46:03.995502: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1003520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/40 [>.............................] - ETA: 15:10 - loss: 0.6957 - accuracy: 0.5625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:46:27.235848: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1003520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1126s 28s/step - loss: 0.5700 - accuracy: 0.7110 - val_loss: 0.4382 - val_accuracy: 0.7925\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 1123s 28s/step - loss: 0.4411 - accuracy: 0.7894 - val_loss: 0.3939 - val_accuracy: 0.8239\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 1121s 28s/step - loss: 0.3936 - accuracy: 0.8332 - val_loss: 0.4193 - val_accuracy: 0.7925\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 1122s 28s/step - loss: 0.3701 - accuracy: 0.8442 - val_loss: 0.3642 - val_accuracy: 0.8553\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 1119s 28s/step - loss: 0.3660 - accuracy: 0.8379 - val_loss: 0.3969 - val_accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "base_model = VGG16(\n",
    "    include_top=False,\n",
    "    input_shape=x.shape\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=x.shape)\n",
    "\n",
    "base = base_model(inputs, training=False)\n",
    "\n",
    "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
    "\n",
    "outputs = keras.layers.Dense(2)(vectors)\n",
    "\n",
    "vgg_model = keras.Model(inputs, outputs)\n",
    "learning_rate = 0.01\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "vgg_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "vgg_history = vgg_model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation - VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1277 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 1118s 28s/step - loss: 2.7026 - accuracy: 0.8418 - val_loss: 0.6868 - val_accuracy: 0.6918\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 1115s 28s/step - loss: 1.4606 - accuracy: 0.8669 - val_loss: 0.5167 - val_accuracy: 0.7170\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 1115s 28s/step - loss: 1.0982 - accuracy: 0.8802 - val_loss: 0.4614 - val_accuracy: 0.7987\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 1115s 28s/step - loss: 0.8392 - accuracy: 0.8951 - val_loss: 0.6712 - val_accuracy: 0.6101\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 1114s 28s/step - loss: 0.7995 - accuracy: 0.8692 - val_loss: 0.4766 - val_accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    './dataset/train',\n",
    "    target_size=(350, 350),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "vgg_history = vgg_model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 161 images belonging to 2 classes.\n",
      "6/6 [==============================] - 125s 20s/step - loss: 1.7290 - accuracy: 0.8882\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_ds = test_gen.flow_from_directory(\n",
    "    './dataset/test',\n",
    "    target_size=(350, 350),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "vgg_score = vgg_model.evaluate(test_ds)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 304s 7s/step - loss: 7.6673 - accuracy: 0.5881 - val_loss: 0.7704 - val_accuracy: 0.4780\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 300s 7s/step - loss: 3.9670 - accuracy: 0.6273 - val_loss: 0.7623 - val_accuracy: 0.4906\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 298s 7s/step - loss: 3.3383 - accuracy: 0.6531 - val_loss: 0.7297 - val_accuracy: 0.5220\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 300s 7s/step - loss: 2.3929 - accuracy: 0.6476 - val_loss: 0.6945 - val_accuracy: 0.5283\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 298s 7s/step - loss: 3.3566 - accuracy: 0.6296 - val_loss: 1.1037 - val_accuracy: 0.4780\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "base_model = InceptionV3(\n",
    "    include_top=False,\n",
    "    input_shape=x.shape\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=x.shape)\n",
    "\n",
    "base = base_model(inputs, training=False)\n",
    "\n",
    "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
    "\n",
    "outputs = keras.layers.Dense(2)(vectors)\n",
    "\n",
    "inception_model = keras.Model(inputs, outputs)\n",
    "learning_rate = 0.01\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "inception_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "inception_history = inception_model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation - Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1277 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 300s 7s/step - loss: 0.7299 - accuracy: 0.6445 - val_loss: 0.5181 - val_accuracy: 0.7799\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 301s 7s/step - loss: 0.4602 - accuracy: 0.7917 - val_loss: 0.4120 - val_accuracy: 0.8365\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 300s 7s/step - loss: 0.4028 - accuracy: 0.8285 - val_loss: 0.3799 - val_accuracy: 0.8302\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 300s 7s/step - loss: 0.3665 - accuracy: 0.8504 - val_loss: 0.3651 - val_accuracy: 0.8176\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 300s 7s/step - loss: 0.3571 - accuracy: 0.8449 - val_loss: 0.3539 - val_accuracy: 0.8113\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    './dataset/train',\n",
    "    target_size=(350, 350),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "inception_history = inception_model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 161 images belonging to 2 classes.\n",
      "6/6 [==============================] - 32s 5s/step - loss: 0.3269 - accuracy: 0.8634\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_ds = test_gen.flow_from_directory(\n",
    "    './dataset/test',\n",
    "    target_size=(350, 350),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "inception_score = inception_model.evaluate(test_ds)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenwu/miniconda3/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "model_performance = {xception_model : xception_score, inception_model : inception_score, vgg_model : vgg_score}\n",
    "max_model = max(model_performance, key=model_performance.get)\n",
    "max_model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Converstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 03:45:10.086320: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0iay6g4e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 03:45:38.055348: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2021-12-07 03:45:38.055395: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2021-12-07 03:45:38.056513: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp0iay6g4e\n",
      "2021-12-07 03:45:38.103121: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2021-12-07 03:45:38.103157: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/tmp0iay6g4e\n",
      "2021-12-07 03:45:38.301867: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2021-12-07 03:45:39.040165: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /tmp/tmp0iay6g4e\n",
      "2021-12-07 03:45:39.348495: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 1291992 microseconds.\n",
      "2021-12-07 03:45:39.984486: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.lite as tflite\n",
    "\n",
    "model = keras.models.load_model('model.h5')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f_out:\n",
    "    f_out.write(tflite_model)\n",
    "\n",
    "interpreter = tflite.Interpreter(model_path='model.tflite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
